{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ac151b-3b3a-4238-9a5b-3e5dca122326",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdea92-a01b-4f20-baa7-e5495f1f63c3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab86fdf-de40-4796-b00c-d4da3e65bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, floor, window, concat_ws\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c90f46d-8d4f-4609-b662-2d9ed30574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession with the Kafka JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTaxiStream\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a407f2b8-4c63-412c-9c90-8a25f05655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType() \\\n",
    "    .add(\"0\", StringType()) \\\n",
    "    .add(\"1\", StringType()) \\\n",
    "    .add(\"2\", TimestampType()) \\\n",
    "    .add(\"3\", TimestampType()) \\\n",
    "    .add(\"4\", DoubleType()) \\\n",
    "    .add(\"5\", DoubleType()) \\\n",
    "    .add(\"6\", DoubleType()) \\\n",
    "    .add(\"7\", DoubleType()) \\\n",
    "    .add(\"8\", DoubleType()) \\\n",
    "    .add(\"9\", DoubleType()) \\\n",
    "    .add(\"10\", StringType()) \\\n",
    "    .add(\"11\", DoubleType()) \\\n",
    "    .add(\"12\", DoubleType()) \\\n",
    "    .add(\"13\", DoubleType()) \\\n",
    "    .add(\"14\", DoubleType()) \\\n",
    "    .add(\"15\", DoubleType()) \\\n",
    "    .add(\"16\", DoubleType())\n",
    "\n",
    "# Define Schema for Incoming Data\n",
    "schema = StructType() \\\n",
    "    .add(\"medallion\", StringType()) \\\n",
    "    .add(\"hack_license\", StringType()) \\\n",
    "    .add(\"pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"trip_time_in_secs\", DoubleType()) \\\n",
    "    .add(\"trip_distance\", DoubleType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"fare_amount\", DoubleType()) \\\n",
    "    .add(\"surcharge\", DoubleType()) \\\n",
    "    .add(\"mta_tax\", DoubleType()) \\\n",
    "    .add(\"tip_amount\", DoubleType()) \\\n",
    "    .add(\"tolls_amount\", DoubleType()) \\\n",
    "    .add(\"total_amount\", DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"taxi-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "parsed_taxi_stream = raw_df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "    .select(from_json(\"json_data\", json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumnRenamed(\"0\", \"medallion\") \\\n",
    "    .withColumnRenamed(\"1\", \"hack_license\") \\\n",
    "    .withColumnRenamed(\"2\", \"pickup_datetime\") \\\n",
    "    .withColumnRenamed(\"3\", \"dropoff_datetime\") \\\n",
    "    .withColumnRenamed(\"4\", \"trip_time_in_secs\") \\\n",
    "    .withColumnRenamed(\"5\", \"trip_distance\") \\\n",
    "    .withColumnRenamed(\"6\", \"pickup_longitude\") \\\n",
    "    .withColumnRenamed(\"7\", \"pickup_latitude\") \\\n",
    "    .withColumnRenamed(\"8\", \"dropoff_longitude\") \\\n",
    "    .withColumnRenamed(\"9\", \"dropoff_latitude\") \\\n",
    "    .withColumnRenamed(\"10\", \"payment_type\") \\\n",
    "    .withColumnRenamed(\"11\", \"fare_amount\") \\\n",
    "    .withColumnRenamed(\"12\", \"surcharge\") \\\n",
    "    .withColumnRenamed(\"13\", \"mta_tax\") \\\n",
    "    .withColumnRenamed(\"14\", \"tip_amount\") \\\n",
    "    .withColumnRenamed(\"15\", \"tolls_amount\") \\\n",
    "    .withColumnRenamed(\"16\", \"total_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bc510-4ddc-45ed-aaf1-8a33fd614d03",
   "metadata": {},
   "source": [
    "# Query 0: Data Cleansing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1dd0a5-68d2-45c0-8842-1768f91c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove malformed and invalid data\n",
    "cleaned_taxi_stream = parsed_taxi_stream \\\n",
    "    .filter(\"medallion IS NOT NULL AND hack_license IS NOT NULL\") \\\n",
    "    .filter(\"pickup_longitude != 0.0 AND pickup_latitude != 0.0\") \\\n",
    "    .filter(\"dropoff_longitude != 0.0 AND dropoff_latitude != 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5968d8ef-d0e3-420d-9f67-256e35d05ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cleaned_taxi_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"taxi_trips_cleaned\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e4abc85-cefc-40c3-a1d6-95f6ba11ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|5EE2C4D3BF57BDB455E74B03B89E43A7|E96EF8F6E6122591F9465376043B946D|2013-01-01 00:00:09|2013-01-01 00:00:36|26.0             |0.1          |-73.99221       |40.725124      |-73.991646       |40.726658       |CSH         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5         |\n",
      "|CA6CD9BAED6A85E430F7BFC0BC84ABD0|77FFDF38272A6006517D53EDA14333E2|2013-01-01 00:00:20|2013-01-01 00:01:22|61.0             |2.2          |-73.9701        |40.768005      |-73.969772       |40.767834       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|15162141EA7436635C696F5BC023D2D6|CDCB7729DE07243726FF7BB0BD5D06BF|2013-01-01 00:00:14|2013-01-01 00:01:37|83.0             |0.2          |-73.975441      |40.749657      |-73.977333       |40.751991       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|025B98A22ED771118FC0EB44A0D3BD9D|7D89374F8E98F30A19F2381EC71A16BA|2013-01-01 00:00:40|2013-01-01 00:01:40|60.0             |0.3          |-74.005165      |40.720531      |-74.003929       |40.725655       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|07290D3599E7A0D62097A346EFCC1FB5|E7750A37CAB07D0DFF0AF7E3573AC141|2013-01-01 00:00:00|2013-01-01 00:02:00|120.0            |0.44         |-73.956528      |40.716976      |-73.96244        |40.715008       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5         |\n",
      "|E79E74C15D90CD93B1564E91E3D64765|145038A0CC99D6982D8001BE668154CA|2013-01-01 00:01:00|2013-01-01 00:02:00|60.0             |0.45         |-73.95208       |40.790169      |-73.948921       |40.794323       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5         |\n",
      "|FD39403FDE46B6C753DDD6518A4365D7|2B36D07A27BB35D7DF7170C83EEAA196|2013-01-01 00:01:00|2013-01-01 00:02:00|60.0             |0.34         |-74.003197      |40.708313      |-74.005608       |40.706551       |CRD         |3.5        |0.5      |0.5    |0.8       |0.0         |5.3         |\n",
      "|E0F370CA508B7E8F639D7FAB3798F399|5AE4F2FB8B90CA06F9AA39FEF82DE2BB|2013-01-01 00:01:35|2013-01-01 00:02:23|48.0             |1.2          |-73.977898      |40.757912      |-73.97802        |40.757362       |DIS         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5         |\n",
      "|08E54F4C460720DDE43460E354486FBC|33276CA24A915CBD668AF96873D07883|2013-01-01 00:00:00|2013-01-01 00:02:44|163.0            |0.5          |-73.999878      |40.743343      |-74.003708       |40.74828        |CRD         |4.0        |0.5      |0.5    |1.0       |0.0         |6.0         |\n",
      "|81124DBD6AB03F24B895D5CF04AAACA2|EFEDEB0551AA8916DE8F1997F0DE3647|2013-01-01 00:02:42|2013-01-01 00:02:50|8.0              |0.0          |-73.781586      |40.644794      |-73.781281       |40.644878       |NOC         |20.0       |0.5      |0.0    |0.0       |0.0         |20.5        |\n",
      "|8287BE67DA22E4ABB34A4E6709402BFF|B19151C3FB6D993FF43431B6A11E3367|2013-01-01 00:02:06|2013-01-01 00:02:54|48.0             |0.0          |-73.991959      |40.726269      |-73.992226       |40.72551        |NOC         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5         |\n",
      "|0CEBE42EAF42C338052FAE8D61612C58|CC7A4176549BA819E07D4A0463F87D2E|2013-01-01 00:00:00|2013-01-01 00:03:00|180.0            |1.56         |-74.00975       |40.706432      |-73.971985       |40.794716       |CSH         |6.5        |0.5      |0.5    |0.0       |0.0         |7.5         |\n",
      "|0EC22AAF491A8BD91F279350C2B010FD|778C92B26AE78A9EBDF96B49C67E4007|2013-01-01 00:01:00|2013-01-01 00:03:00|120.0            |0.71         |-73.973145      |40.752827      |-73.965897       |40.760445       |CSH         |4.0        |0.5      |0.5    |0.0       |0.0         |5.0         |\n",
      "|1390FB380189DF6BBFDA4DC847CAD14F|BE317B986700F63C43438482792C8654|2013-01-01 00:01:00|2013-01-01 00:03:00|120.0            |0.48         |-74.004173      |40.720947      |-74.003838       |40.726189       |CSH         |4.0        |0.5      |0.5    |0.0       |0.0         |5.0         |\n",
      "|319AE2555940BA65DB0749E1DD1FBA0B|BAC146F5AA74DE3040A5D53572EA663A|2013-01-01 00:00:00|2013-01-01 00:03:00|180.0            |0.39         |-73.990608      |40.734997      |-73.989487       |40.730324       |CRD         |4.0        |0.5      |0.5    |0.9       |0.0         |5.9         |\n",
      "|3B4129883A1D05BE89F2C929DE136281|7077F9FD5AD649AEACA4746B2537E3FA|2013-01-01 00:01:00|2013-01-01 00:03:00|120.0            |0.61         |-73.987373      |40.724861      |-73.983772       |40.730995       |CRD         |4.0        |0.5      |0.5    |0.0       |0.0         |5.0         |\n",
      "|3E7DB7D3DC9961BA3A304E5B4B419E5F|452B322CA3BB3132FF0F59FADAE615D6|2013-01-01 00:00:00|2013-01-01 00:03:00|180.0            |1.52         |-73.954117      |40.778343      |-73.941818       |40.795479       |CSH         |6.0        |0.5      |0.5    |0.0       |0.0         |7.0         |\n",
      "|557B02E4BC71B16C0EBC9CEE6F757822|10DEF6B15C01194B2D4933C638926EEF|2013-01-01 00:01:00|2013-01-01 00:03:00|120.0            |0.41         |-73.976173      |40.744942      |-73.980919       |40.747517       |CSH         |4.0        |0.5      |0.5    |0.0       |0.0         |5.0         |\n",
      "|5CC9B3C9725FCD7FAE490B4C614D57EE|9F4A98FA581907FC25B5C1A9A5361365|2013-01-01 00:00:00|2013-01-01 00:03:00|180.0            |0.64         |-74.001404      |40.72261       |-73.998177       |40.729485       |CSH         |4.5        |0.5      |0.5    |0.0       |0.0         |5.5         |\n",
      "|BE4A6F08A4E9CA26DEBBD511A15E5D89|B6B300DB052529A0A9A238DE3C5C29E4|2013-01-01 00:00:00|2013-01-01 00:03:00|180.0            |0.9          |-73.946136      |40.781349      |-73.944977       |40.790836       |CSH         |4.5        |0.5      |0.5    |0.0       |0.0         |5.5         |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View cleansed stream in notebook\n",
    "spark.sql(\"SELECT * FROM taxi_trips_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555b6a1-5a5c-4d2c-b392-79977f2b3c89",
   "metadata": {},
   "source": [
    "# Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f88189-8394-463e-9114-c6aad07b6c75",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37e0f61-17e8-4e07-a543-fec9aa875975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ba2a62-05a0-46d4-9617-a9876b8e08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_lat = 41.474937\n",
    "reference_lon = -74.913585\n",
    "total_cells = 300\n",
    "\n",
    "# Cell sizes from http://www.debs2015.org/call-grand-challenge.html\n",
    "cell_size_lat_deg = 0.004491556  # 500m south\n",
    "cell_size_lon_deg = 0.005986     # 500m east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cc6b1a-4ad4-4e4d-a7c8-caf0d2c6a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate how far the cell location is from the origin\n",
    "def get_cell_id(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    try:\n",
    "        dx = int((lon - reference_lon) / cell_size_lon_deg) + 1 # how many cells east\n",
    "        dy = int((reference_lat - lat) / cell_size_lat_deg) + 1 # how many cells south\n",
    "        if 1 <= dx <= total_cells and 1 <= dy <= total_cells: # validate\n",
    "            return f\"{dx}.{dy}\"\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# create spark udf\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c843e763-11a8-4e28-80b7-9da7334fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cleaned taxi stream data and convert pickup and dropoff locations into start and end cell IDs\n",
    "stream_with_cells = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"end_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL AND end_cell_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ea44ef-8517-4a7f-842b-266c1768f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rides for each route in the last 30 minutes \n",
    "frequent_routes = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\"start_cell_id\", \"end_cell_id\", \"count\") \\\n",
    "    .orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3219b4-8d30-4041-83df-2e24cb3df4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f12bbdb9250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store results\n",
    "frequent_routes.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"top_routes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01aa8050-4b84-42e4-ba22-467f860def02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+\n",
      "|start_cell_id|end_cell_id|count|\n",
      "+-------------+-----------+-----+\n",
      "|156.163      |155.162    |5    |\n",
      "|155.161      |155.161    |5    |\n",
      "|155.159      |155.159    |4    |\n",
      "|153.162      |154.161    |3    |\n",
      "|155.167      |156.166    |3    |\n",
      "|160.157      |161.156    |3    |\n",
      "|164.160      |164.160    |3    |\n",
      "|152.168      |152.167    |3    |\n",
      "|157.159      |158.160    |3    |\n",
      "|155.169      |155.169    |3    |\n",
      "+-------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM top_routes LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9299a-ce45-455c-a4a6-311f47ac8c48",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95fb52de-45d9-4fe8-a4b4-fd2825e3b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f26ad3-33f8-453d-8883-cbee723e229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate delay\n",
    "def process_batch(df, epoch_id):\n",
    "    # Current time for delay calculation\n",
    "    output_time = time.time() * 1000  # Convert to milliseconds\n",
    "    \n",
    "    if df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Get the latest event that triggered the update\n",
    "    latest_event = df.orderBy(col(\"processing_time\").desc()).first()\n",
    "    \n",
    "    # Calculate delay\n",
    "    delay = output_time - latest_event[\"processing_time\"]\n",
    "    \n",
    "    # Get current top 10 routes\n",
    "    top_routes = spark.sql(\"\"\"\n",
    "        SELECT start_cell_id, end_cell_id, count\n",
    "        FROM frequent_routes\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Prepare the result with the 10 most frequent routes\n",
    "    result = {\n",
    "        \"pickup_datetime\": latest_event[\"pickup_datetime\"],\n",
    "        \"dropoff_datetime\": latest_event[\"dropoff_datetime\"],\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    \n",
    "    # Add all 10 routes (or NULL if fewer than 10 available)\n",
    "    for i in range(10):\n",
    "        if i < len(top_routes):\n",
    "            result[f\"start_cell_id_{i+1}\"] = top_routes[i][\"start_cell_id\"]\n",
    "            result[f\"end_cell_id_{i+1}\"] = top_routes[i][\"end_cell_id\"]\n",
    "        else:\n",
    "            result[f\"start_cell_id_{i+1}\"] = None\n",
    "            result[f\"end_cell_id_{i+1}\"] = None\n",
    "    \n",
    "    # Create a DataFrame with the result\n",
    "    result_df = spark.createDataFrame([result])\n",
    "    \n",
    "    # Append to results table\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"top_routes_results\")\n",
    "\n",
    "# Add processing time column for delay calculation - fixed syntax\n",
    "cleaned_taxi_stream = cleaned_taxi_stream.withColumn(\n",
    "    \"processing_time\", unix_timestamp() * 1000\n",
    ")\n",
    "\n",
    "# Process stream with cell IDs\n",
    "stream_with_cells = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"end_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL AND end_cell_id IS NOT NULL\")\n",
    "\n",
    "# Calculate frequent routes - store in memory table\n",
    "frequent_routes_query = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\"),\n",
    "        col(\"count\")\n",
    "    ) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"frequent_routes_new\") \\\n",
    "    .start()\n",
    "\n",
    "# Create a trigger to monitor changes in the stream\n",
    "# This will trigger the output whenever any new data comes in that could affect the top 10\n",
    "change_monitor = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c568a612-63fa-4333-9ba8-fa80ef095407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `top_routes_results` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 9;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['dropoff_datetime DESC NULLS LAST], true\n      +- 'Project ['pickup_datetime, 'dropoff_datetime, 'start_cell_id_1, 'end_cell_id_1, 'start_cell_id_2, 'end_cell_id_2, 'start_cell_id_3, 'end_cell_id_3, 'start_cell_id_4, 'end_cell_id_4, 'start_cell_id_5, 'end_cell_id_5, 'start_cell_id_6, 'end_cell_id_6, 'start_cell_id_7, 'end_cell_id_7, 'start_cell_id_8, 'end_cell_id_8, 'start_cell_id_9, 'end_cell_id_9, 'start_cell_id_10, 'end_cell_id_10, 'delay]\n         +- 'UnresolvedRelation [top_routes_results], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Query to view the results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    SELECT \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        pickup_datetime, dropoff_datetime,\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m        start_cell_id_1, end_cell_id_1,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        start_cell_id_2, end_cell_id_2,\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m        start_cell_id_3, end_cell_id_3,\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m        start_cell_id_4, end_cell_id_4,\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m        start_cell_id_5, end_cell_id_5,\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m        start_cell_id_6, end_cell_id_6,\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m        start_cell_id_7, end_cell_id_7,\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m        start_cell_id_8, end_cell_id_8,\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m        start_cell_id_9, end_cell_id_9,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m        start_cell_id_10, end_cell_id_10,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m        delay\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    FROM top_routes_results\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m    ORDER BY dropoff_datetime DESC\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m    LIMIT 10\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `top_routes_results` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 9;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['dropoff_datetime DESC NULLS LAST], true\n      +- 'Project ['pickup_datetime, 'dropoff_datetime, 'start_cell_id_1, 'end_cell_id_1, 'start_cell_id_2, 'end_cell_id_2, 'start_cell_id_3, 'end_cell_id_3, 'start_cell_id_4, 'end_cell_id_4, 'start_cell_id_5, 'end_cell_id_5, 'start_cell_id_6, 'end_cell_id_6, 'start_cell_id_7, 'end_cell_id_7, 'start_cell_id_8, 'end_cell_id_8, 'start_cell_id_9, 'end_cell_id_9, 'start_cell_id_10, 'end_cell_id_10, 'delay]\n         +- 'UnresolvedRelation [top_routes_results], [], false\n"
     ]
    }
   ],
   "source": [
    "# Query to view the results\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_datetime, dropoff_datetime,\n",
    "        start_cell_id_1, end_cell_id_1,\n",
    "        start_cell_id_2, end_cell_id_2,\n",
    "        start_cell_id_3, end_cell_id_3,\n",
    "        start_cell_id_4, end_cell_id_4,\n",
    "        start_cell_id_5, end_cell_id_5,\n",
    "        start_cell_id_6, end_cell_id_6,\n",
    "        start_cell_id_7, end_cell_id_7,\n",
    "        start_cell_id_8, end_cell_id_8,\n",
    "        start_cell_id_9, end_cell_id_9,\n",
    "        start_cell_id_10, end_cell_id_10,\n",
    "        delay\n",
    "    FROM top_routes_results\n",
    "    ORDER BY dropoff_datetime DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698a67",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138b680-2314-4517-8694-9e2013bf7205",
   "metadata": {},
   "source": [
    "# Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71532313-5144-4e9f-b28d-8c55da28057d",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52959a9c-7791-4604-a655-bcf5cc28e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import window, max as max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3425dc-2fc8-4b67-977f-3d5816f61051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide query 1 degrees by 2\n",
    "cell_size_lat_deg = 0.002245778   # 250m south\n",
    "cell_size_lon_deg = 0.002993      # 250m east\n",
    "total_cells = 600\n",
    "\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6d0e03b-c21a-4b3f-888d-0276a873cce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f133fce9490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profit_stream = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL\") \\\n",
    "    .withColumn(\"profit\", when((col(\"fare_amount\") >= 0) & (col(\"tip_amount\") >= 0), col(\"fare_amount\") + col(\"tip_amount\"))) \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"15 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"15 minutes\"), col(\"start_cell_id\")) \\\n",
    "    .agg(\n",
    "        expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"),\n",
    "        max_(\"pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "        max_(\"dropoff_datetime\").alias(\"dropoff_datetime\")\n",
    "    ) \\\n",
    "    .selectExpr(\"start_cell_id as cell_id\", \"median_profit\", \"window.end as profit_window_end\", \"pickup_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "# Write to Delta with partitioning by cell_id\n",
    "profit_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .queryName(\"profit_stream\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"cell_id\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/profit_stream\") \\\n",
    "    .option(\"path\", \"./output/profitable_areas_delta\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "441c3a95-50a2-45bf-823c-ca598bc7f730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f133fcefe10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty_taxies_stream by saving pickup table \n",
    "cleaned_taxi_stream \\\n",
    "    .select(\"medallion\", \"pickup_datetime\") \\\n",
    "    .filter(\"medallion IS NOT NULL\") \\\n",
    "    .withWatermark(\"pickup_datetime\", \"30 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"pickup_table\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "pickup_static = spark.read.table(\"pickup_table\")\n",
    "\n",
    "# get all drop off in last 30 minutes\n",
    "dropoffs = cleaned_taxi_stream \\\n",
    "    .withColumn(\"dropoff_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"dropoff_cell_id IS NOT NULL\") \\\n",
    "    .select(\"medallion\", \"dropoff_datetime\", \"dropoff_cell_id\") \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\")\n",
    "\n",
    "# Calculate empty taxis based on datetimes\n",
    "empty_taxis = dropoffs.alias(\"d\").join(\n",
    "    pickup_static.alias(\"p\"),\n",
    "    (col(\"d.medallion\") == col(\"p.medallion\")) & (col(\"p.pickup_datetime\") > col(\"d.dropoff_datetime\")),\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Calculate empty taxis per cell\n",
    "empty_taxis_per_cell = empty_taxis \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"30 minutes\"), col(\"dropoff_cell_id\")) \\\n",
    "    .agg(expr(\"approx_count_distinct(medallion)\").alias(\"empty_taxis\")) \\\n",
    "    .selectExpr(\"dropoff_cell_id as cell_id\", \"empty_taxis\", \"window.end as empty_window_end\")\n",
    "\n",
    "# Write empty taxis to memory\n",
    "empty_taxis_per_cell.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"empty_taxis_stream\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36648758-ecb5-4cdd-b147-d74cd842032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f133fce8d10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join together and process batches\n",
    "def process_batch(batch_df, batch_id):\n",
    "    result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.pickup_datetime, p.dropoff_datetime, p.cell_id AS profitable_cell_id,\n",
    "            e.empty_taxis AS empty_taxies_in_cell, \n",
    "            p.median_profit AS median_profit_in_cell,\n",
    "            CASE \n",
    "                WHEN e.empty_taxis = 0 THEN NULL \n",
    "                ELSE p.median_profit / e.empty_taxis \n",
    "            END AS profitability_of_cell\n",
    "        FROM profit_stream p\n",
    "        JOIN empty_taxis_stream e ON p.cell_id = e.cell_id\n",
    "    \"\"\")\n",
    "    result_df.show(truncate=False, n=50)\n",
    "\n",
    "    # Write the results to Delta or Parquet\n",
    "    result_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"./output/profitable_areas_parquet\")\n",
    "\n",
    "# Write processed batch to Delta or Parquet\n",
    "cleaned_taxi_stream.selectExpr(\"CAST(NULL AS STRING) as test\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581993ef-868c-4057-9628-ea4779986521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77615fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:18:00|2013-01-01 00:22:00|316.324           |2                   |7.0                  |3.5                  |\n",
      "|2013-01-01 00:26:00|2013-01-01 00:29:00|321.314           |24                  |10.5                 |0.4375               |\n",
      "|2013-01-01 00:14:00|2013-01-01 00:27:00|314.304           |10                  |13.0                 |1.3                  |\n",
      "|2013-01-01 00:12:39|2013-01-01 00:14:33|312.310           |15                  |5.5                  |0.36666666666666664  |\n",
      "|2013-01-01 00:19:00|2013-01-01 00:28:00|303.344           |3                   |7.5                  |2.5                  |\n",
      "|2013-01-01 00:06:26|2013-01-01 00:14:37|303.327           |16                  |6.5                  |0.40625              |\n",
      "|2013-01-01 00:20:13|2013-01-01 00:29:00|312.311           |13                  |8.5                  |0.6538461538461539   |\n",
      "|2013-01-01 00:16:02|2013-01-01 00:24:01|322.334           |1                   |7.5                  |7.5                  |\n",
      "|2013-01-01 00:09:15|2013-01-01 00:11:01|321.315           |19                  |4.0                  |0.21052631578947367  |\n",
      "|2013-01-01 00:19:00|2013-01-01 00:23:00|303.342           |7                   |5.0                  |0.7142857142857143   |\n",
      "|2013-01-01 00:23:00|2013-01-01 00:29:00|322.309           |14                  |12.1                 |0.8642857142857142   |\n",
      "|2013-01-01 00:26:04|2013-01-01 00:28:58|306.325           |6                   |8.5                  |1.4166666666666667   |\n",
      "|2013-01-01 00:09:35|2013-01-01 00:14:19|316.317           |8                   |4.5                  |0.5625               |\n",
      "|2013-01-01 00:09:13|2013-01-01 00:13:46|311.333           |19                  |7.5                  |0.39473684210526316  |\n",
      "|2013-01-01 00:23:00|2013-01-01 00:29:00|313.327           |12                  |7.5                  |0.625                |\n",
      "|2013-01-01 00:21:00|2013-01-01 00:28:15|308.323           |13                  |11.0                 |0.8461538461538461   |\n",
      "|2013-01-01 00:07:00|2013-01-01 00:19:00|308.316           |3                   |11.5                 |3.8333333333333335   |\n",
      "|2013-01-01 00:13:00|2013-01-01 00:23:00|321.356           |1                   |10.9                 |10.9                 |\n",
      "|2013-01-01 00:20:55|2013-01-01 00:26:53|316.315           |9                   |6.5                  |0.7222222222222222   |\n",
      "|2013-01-01 00:05:56|2013-01-01 00:09:26|315.325           |15                  |6.0                  |0.4                  |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Read the Delta table that was written\n",
    "# df = spark.read.format(\"delta\").load(\"./output/profitable_areas_delta\")\n",
    "\n",
    "# # Show the top rows of the Delta data\n",
    "# df.show(truncate=False)\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(\"./output/profitable_areas_parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa2fd1",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c4ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a11224f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_profit(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        return\n",
    "\n",
    "    import time\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Get current time in ms\n",
    "    output_time = time.time() * 1000\n",
    "\n",
    "    # Get latest record for delay calculation\n",
    "    latest_event = df.orderBy(col(\"processing_time\").desc()).first()\n",
    "    delay = output_time - latest_event[\"processing_time\"]\n",
    "\n",
    "    # Get top 10 profitable areas\n",
    "    top_areas = spark.sql(\"\"\"\n",
    "        SELECT cell_id, empty_taxis, median_profit, profitability\n",
    "        FROM profitable_areas\n",
    "        ORDER BY profitability DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # Construct result row\n",
    "    result = {\n",
    "        \"pickup_datetime\": latest_event[\"pickup_datetime\"],\n",
    "        \"dropoff_datetime\": latest_event[\"dropoff_datetime\"],\n",
    "        \"delay\": delay\n",
    "    }\n",
    "\n",
    "    for i in range(10):\n",
    "        if i < len(top_areas):\n",
    "            result[f\"cell_id_{i+1}\"] = top_areas[i][\"cell_id\"]\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = top_areas[i][\"empty_taxis\"]\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = top_areas[i][\"median_profit\"]\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = top_areas[i][\"profitability\"]\n",
    "        else:\n",
    "            result[f\"cell_id_{i+1}\"] = None\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = None\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = None\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    # Convert to DataFrame and write to Delta Lake\n",
    "    result_df = spark.createDataFrame([result])\n",
    "    \n",
    "    result_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"./output/top_profitable_areas_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33ea2f54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profitability_with_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprofitability_with_time\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch_profit) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 second\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profitability_with_time' is not defined"
     ]
    }
   ],
   "source": [
    "profitability_with_time.writeStream \\\n",
    "    .foreachBatch(process_batch_profit) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"5 second\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5675da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = spark.read.format(\"delta\").load(\"./output/top_profitable_areas_delta\")\n",
    "top10_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
